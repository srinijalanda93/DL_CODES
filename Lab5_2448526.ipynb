{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17afa480-911c-4b62-8a5a-078843bf4ce7",
   "metadata": {},
   "source": [
    "**\n",
    "\n",
    "### **Step 1: Choose and Load a Dataset**\n",
    "- Pick a dataset (e.g., CIFAR-10, Flowers, Fruits, or Animals).  \n",
    "- You can download a dataset from **Kaggle** or use built-in ones from **TensorFlow/Keras** (`tf.keras.datasets`).  \n",
    "- Load the dataset and split it into **training** and **testing** sets.  \n",
    "\n",
    "### **Step 2: Load Pre-trained VGG16 Model**\n",
    "- Use `keras.applications.VGG16` with **pre-trained ImageNet weights**.  \n",
    "- **Remove the top layer** (fully connected layer for classification).  \n",
    "- **Add a new fully connected layer** that matches your dataset‚Äôs number of classes.  \n",
    "\n",
    "### **Step 3: Preprocess Images**\n",
    "- Resize all images to **224√ó224 pixels** (VGG16 input size).  \n",
    "- Normalize pixel values **between 0 and 1** (divide by 255).  \n",
    "- Use `ImageDataGenerator` for easy preprocessing.  \n",
    "\n",
    "### **Step 4: Freeze Convolutional Layers & Train Custom Layers**\n",
    "- **Freeze all convolutional layers** (so they don‚Äôt update weights).  \n",
    "- Train **only the newly added fully connected layers**.  \n",
    "- Use **Adam or SGD optimizer**.  \n",
    "- Train for **at least 5‚Äì10 epochs** and observe accuracy.  \n",
    "\n",
    "### **Step 5: Evaluate Model Performance**\n",
    "- Use **test set** to check accuracy and loss.  \n",
    "- Generate **a confusion matrix** to analyze predictions.  \n",
    "- Generate **a classification report** (precision, recall, F1-score).  \n",
    "\n",
    "### **Step 6: Apply Data Augmentation**\n",
    "- Use `ImageDataGenerator` for:  \n",
    "  - **Rotation**  \n",
    "  - **Flipping**  \n",
    "  - **Zooming**  \n",
    "- Train the model again and compare results with/without augmentation.  \n",
    "\n",
    "### **Step 7: Compare VGG16 with Another Model**\n",
    "- Choose **another pre-trained model** (e.g., MobileNet or ResNet).  \n",
    "- Follow the **same process** (load, modify, train, and evaluate).  \n",
    "- Compare **accuracy, speed, and performance**.  \n",
    "\n",
    "### **Step 8: Submit the Notebook**\n",
    "- **Summarize findings** in a Jupyter Notebook (`.ipynb`).  \n",
    "- Show **comparison grow if you need code snippets or any clarifications! üöÄüî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4782a4fd-2823-49bc-8400-b95469a9025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16, MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d4243a7-676c-4f03-ae4e-95b0a1b95804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load CIFAR-10 Dataset\n",
    "dataset = tf.keras.datasets.cifar10\n",
    "(x_train, y_train), (x_test, y_test) = dataset.load_data()\n",
    "\n",
    "# Use a smaller subset\n",
    "subset_size = 1000  # Choose a smaller number\n",
    "x_train, y_train = x_train[:subset_size], y_train[:subset_size]\n",
    "x_test, y_test = x_test[:subset_size // 10], y_test[:subset_size // 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b372741c-024e-41ad-a06e-8e1f15d6da8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1000, 32, 32, 3)\n",
      "Test set shape: (100, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set shape: {x_train.shape}\")\n",
    "print(f\"Test set shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b1f22b-b5bd-40c3-8a06-384eb32f408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize images to 224x224 (VGG16 input size)\n",
    "x_train = np.array([tf.image.resize(img, (224, 224)).numpy() for img in x_train])\n",
    "x_test = np.array([tf.image.resize(img, (224, 224)).numpy() for img in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c23f1121-1c67-42be-b71b-23805b3a5dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 224, 224, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fbc6a84-030b-421b-9436-a765f4ad69ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d27e03d-7ee3-4dac-aa3c-caea73a2c2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 224, 224, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02795261-acfc-4445-9ce9-829ac4c1953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load Pretrained VGG16 (without top layer)\n",
    "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "vgg16_base.trainable = False  # Freeze convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f62d997e-1c64-4a94-bc60-b39ffc7c81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Add Custom Fully Connected Layers\n",
    "x = Flatten()(vgg16_base.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "out_layer = Dense(10, activation='softmax')(x)  # 10 classes in CIFAR-10\n",
    "\n",
    "model = Model(inputs=vgg16_base.input, outputs=out_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b17fdc7a-e3f0-4879-84aa-7eb617b9440e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 13s/step - accuracy: 0.1100 - loss: 3.3420 - val_accuracy: 0.2500 - val_loss: 2.1433\n",
      "Epoch 2/3\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1420s\u001b[0m 45s/step - accuracy: 0.1798 - loss: 2.1606 - val_accuracy: 0.3100 - val_loss: 1.9544\n",
      "Epoch 3/3\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 16s/step - accuracy: 0.2437 - loss: 2.0570 - val_accuracy: 0.3300 - val_loss: 1.9391\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Compile Model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Step 5: Train the Model\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d8f55-7773-4456-afcf-8f1cfdff0d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Evaluate Model Performance\n",
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c30f29-19d9-4016-8bcb-c51dd229ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Classification Report & Confusion Matrix\n",
    "y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "print(classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f4c39e-1884-4ee5-9e76-9c1649f00862",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 7: Apply Data Augmentation\n",
    "data_gen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2\n",
    ")\n",
    "\n",
    "train_aug = data_gen.flow(x_train, y_train, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e49e7e-82d0-4dae-96e4-4ed49223bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Again with Augmentation\n",
    "model.fit(train_aug, validation_data=(x_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# Step 8: Compare with MobileNetV2\n",
    "mobilenet_base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "mobilenet_base.trainable = False\n",
    "\n",
    "x = Flatten()(mobilenet_base.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "out_layer = Dense(10, activation='softmax')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d86f424-4668-4a61-9001-b48554d773a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_model = Model(inputs=mobilenet_base.input, outputs=out_layer)\n",
    "mobilenet_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "# Train MobileNet Model\n",
    "mobilenet_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad34d88-87be-48bc-bfd1-1ae9de67f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Performance\n",
    "loss_vgg16, acc_vgg16 = model.evaluate(x_test, y_test)\n",
    "loss_mobilenet, acc_mobilenet = mobilenet_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f\"VGG16 Test Accuracy: {acc_vgg16:.4f}\")\n",
    "print(f\"MobileNetV2 Test Accuracy: {acc_mobilenet:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
